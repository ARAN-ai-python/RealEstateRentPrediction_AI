{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARAN1218/RealEstateRentPrediction_AI/blob/main/jupyter%20notebook/SUUMO%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%94%E3%83%B3%E3%82%B0_%E6%94%B9%E5%96%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a7d9f3f",
      "metadata": {
        "id": "2a7d9f3f"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリをインポート\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from time import sleep\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "#後でformatでページ数を代入するので、urlの内「pn=」の部分は「pn={}」としておく\n",
        "urls = {\n",
        "    #さがキャン\n",
        "    '相模原':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=14&sc=14151&sc=14152&sc=14153&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '町田':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13209&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '横浜':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?initFlg=1&seniFlg=1&pc=30&ar=030&ta=14&sa=01&newflg=0&km=1&bs=040&pn={}\",\n",
        "    \n",
        "    #青キャン\n",
        "    '渋谷':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13113&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '新宿':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?initFlg=1&seniFlg=1&pc=30&ar=030&ta=13&scTmp=13104&kb=0&xb=0&newflg=0&km=1&sc=13104&bs=040&pn={}\",\n",
        "    '港':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13103&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '品川':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13109&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '目黒':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13110&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '世田谷':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13112&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '杉並':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13115&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\",\n",
        "    '中野':\"https://suumo.jp/jj/common/ichiran/JJ901FC004/?ar=030&ta=13&sc=13114&kwd=&cb=0.0&ct=9999999&kb=0&kt=9999999&km=1&xb=0&xt=9999999&et=9999999&cn=9999999&newflg=0&pn={}\"\n",
        "}\n",
        "\n",
        "\n",
        "#不動産データスクレイピングのメインプログラム\n",
        "#Real_Estate_Data_Scraping\n",
        "def REDS(start,end,place,pre_results=[]):\n",
        "    #初期値\n",
        "    d_list = pre_results\n",
        "    url = urls[place]\n",
        "    \n",
        "    #pre_resultsにデータが入っていた場合、進捗管理リストのprogressにページの番号を入れておく。\n",
        "    if len(d_list) > 0:\n",
        "        progress = list(pd.DataFrame(d_list)['ページ'].unique())\n",
        "    else:\n",
        "        progress = []\n",
        "    \n",
        "    #range(start,end+1,2)とすれば、2の部分を消せば指定した全ページのデータが取得できる。\n",
        "    for i in tqdm(range(start,end+1)):\n",
        "        \n",
        "        if i in progress:\n",
        "            continue\n",
        "        \n",
        "        #途中でエラーが発生してもそれまでの結果が保存できるようにする。\n",
        "        try:\n",
        "            \n",
        "            #ページを遷移させる\n",
        "            target_url = url.format(i)\n",
        "            \n",
        "            #bot対策の対策として、ヘッダーを偽装する\n",
        "            headers = {\n",
        "                \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\"\n",
        "            }\n",
        "\n",
        "            #Requestsを用いてtarget_urlにアクセスする\n",
        "            r = requests.get(target_url, headers=headers)\n",
        "\n",
        "            #サーバー負荷軽減の為、ループ毎に1秒間隔を空ける\n",
        "            sleep(1)\n",
        "\n",
        "            #取得したHTMLをBeautifulSoupで解析する\n",
        "            soup = BeautifulSoup(r.text)\n",
        "\n",
        "            #BeautifulSoupで解析したHTMLの内、欲しい不動産情報が乗っている部分を取得\n",
        "            contents = soup.find_all('div',class_='cassettebox js-normalLink js-cassetLink')\n",
        "\n",
        "            #1ページ当たり30件の不動産データが表示されていれば、リストcontentsは30個の要素を持つはずなので、それらを一つずつ取り出してデータを取得する。\n",
        "            for content in contents:\n",
        "\n",
        "                #再開機能の為、取得元のページ数を保存しておく\n",
        "                pages = i\n",
        "\n",
        "                #上の行のデータを取得\n",
        "                rows = content.find_all('table',class_='listtable')\n",
        "                address = rows[0].find_all('div',class_='infodatabox-box-txt')[0].text\n",
        "                station = rows[0].find_all('div',class_='infodatabox-box-txt')[1].text\n",
        "                access = rows[0].find_all('div',class_='infodatabox-box-txt')[2].text\n",
        "\n",
        "                #下の行のデータを取得\n",
        "                r_fees = rows[1].find_all('dd',class_='infodatabox-details-txt')[0].text[:-2]\n",
        "                mc_fees = rows[1].find_all('dd',class_='infodatabox-details-txt')[1].text[:-1]\n",
        "                k_fees = rows[1].find_all('dd',class_='infodatabox-details-txt')[2].text.split('/')[0]\n",
        "                s_fees = rows[1].find_all('dd',class_='infodatabox-details-txt')[2].text.split('/')[1][:-2]\n",
        "                area = rows[1].find_all('dd',class_='infodatabox-details-txt')[3].text[:-2]\n",
        "                layout = rows[1].find_all('dd',class_='infodatabox-details-txt')[4].text\n",
        "                age = rows[1].find_all('div',class_='infodatabox-box-txt')[2].text\n",
        "\n",
        "                #取得した各種データを辞書dに格納する\n",
        "                d = {\n",
        "                'ページ':pages,\n",
        "                '住所':address,\n",
        "                '路線':station,\n",
        "                '交通':access,\n",
        "                '賃料':r_fees,\n",
        "                '管理共益費':mc_fees,\n",
        "                '礼金':k_fees,\n",
        "                '敷金':s_fees,\n",
        "                '専有面積':area,\n",
        "                '間取り':layout,\n",
        "                '築年数':age\n",
        "                }\n",
        "\n",
        "                #辞書dのデータをリストd_listに格納する\n",
        "                d_list.append(d)\n",
        "\n",
        "                #重複したデータを削除する\n",
        "                #d_list = list(map(json.loads,set(map(json.dumps,d_list))))\n",
        "\n",
        "            #進捗を報告させる\n",
        "            print(\"d_list's progress:\",i,\"page　　\",len(d_list))\n",
        "            print(target_url)\n",
        "            \n",
        "        #リストにある存在しないページにアクセスした場合、そのページの読み込みをスキップする。\n",
        "        except IndexError:\n",
        "            continue\n",
        "            \n",
        "        #スクレイピングが中断されても、その時点までに読み込んだデータを出力できるようにする。\n",
        "        except:\n",
        "            break\n",
        "    \n",
        "    #スクレイピングが終わった事を通知\n",
        "    print('Scraping Completed!')\n",
        "    return d_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee142fe",
      "metadata": {
        "id": "2ee142fe"
      },
      "outputs": [],
      "source": [
        "#進捗リセット\n",
        "reds_pre_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5624d74b",
      "metadata": {
        "scrolled": true,
        "id": "5624d74b"
      },
      "outputs": [],
      "source": [
        "#スクレイピングに必要なパラメータを入力\n",
        "start = 1  #初めのページ数\n",
        "end = 613  #終わりのページ数\n",
        "place = '町田'  #(辞書urlsに入っている内の)読み込む地域\n",
        "\n",
        "reds_test = REDS(start,end,place,reds_pre_results)\n",
        "reds_pre_results = reds_test\n",
        "pd.DataFrame(reds_pre_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b60292f",
      "metadata": {
        "id": "4b60292f"
      },
      "outputs": [],
      "source": [
        "#読み込んだページ数\n",
        "pd.DataFrame(reds_test)['ページ'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37341410",
      "metadata": {
        "id": "37341410"
      },
      "source": [
        "print(len(reds_pre_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687b1750",
      "metadata": {
        "scrolled": true,
        "id": "687b1750"
      },
      "outputs": [],
      "source": [
        "reds_df = pd.DataFrame(reds_test)\n",
        "#display(reds_dumped_df.head(10))\n",
        "#reds_dumped_df = reds_df.drop_duplicates()\n",
        "print(len(reds_df))\n",
        "#print(len(reds_dumped_df))\n",
        "#reds_dumped_df.drop(['ページ'],axis=1,inplace=True)\n",
        "#reds_dumped_df.reset_index(drop=True,inplace=True)\n",
        "#print(len(reds_dumped_df))\n",
        "display(reds_df.head(10))\n",
        "\n",
        "#reds_dumped_df['交通'].unique()\n",
        "#reds_df.to_pickle('SUUMO_ _Bigdata.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "257d44ee",
      "metadata": {
        "id": "257d44ee"
      },
      "source": [
        "print(len(reds_pre_results))\n",
        "print(len(reds_dumped_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08ac0a1",
      "metadata": {
        "scrolled": false,
        "id": "a08ac0a1"
      },
      "outputs": [],
      "source": [
        "#相模原のデータを1200ページ集めて、リークを防ぎつつデータ量で暴力振るった。\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Sagamihara_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Machida_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Minato_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Shibuya_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Yokohama_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Shinjuku_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Shinagawa_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Meguro_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Setagaya_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Suginami_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Nakano_Bigdata.pickle')\n",
        "\n",
        "reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Sagakyan_Bigdata.pickle')\n",
        "#reds_dumping = pd.read_pickle('SUUMO_Bigdata/SUUMO_Aokyan_Bigdata.pickle')\n",
        "\n",
        "print(\"処理前データ数：\",len(reds_dumping))\n",
        "#display(reds_dumping.head(10))\n",
        "reds_dumping.drop('ページ',axis=1,inplace=True)\n",
        "reds_dumping = reds_dumping.drop_duplicates()\n",
        "print(\"処理後データ数：\",len(reds_dumping))\n",
        "\n",
        "#reds_dumping.drop([\"路線\"],axis=1,inplace=True)\n",
        "\n",
        "df,df_t = train_test_split(reds_dumping,test_size=0.25,random_state=71,shuffle=True)\n",
        "print(\"処理後学習データ数：\",len(df))\n",
        "print(\"処理後テストデータ数：\",len(df_t))\n",
        "#display(df.head(10))\n",
        "#df_t = reds_dumping[reds_dumping.index <= len(reds_dumping)/4]\n",
        "print(sorted(df['賃料'].unique().astype(float)),\"\\n\")\n",
        "print(sorted(df_t['賃料'].unique().astype(float)))\n",
        "df = df.drop_duplicates()\n",
        "df_t = df_t.drop_duplicates()\n",
        "df_t.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5963f4ef",
      "metadata": {
        "id": "5963f4ef"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリをインポート\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#pandasのデータフレームの列表示制限を解除\n",
        "#pd.set_option('display.max_rows',None)\n",
        "\n",
        "\n",
        "#既にREDSを用いて変数d_listとd_t_listにスクレイピングしたデータが保存され、それぞれdf,df_testにデータフレーム型に変換されて代入されているとする\n",
        "#d_list = REDS(start,end,place)\n",
        "#d_t_list = REDS(start+1,end+1,place)\n",
        "\n",
        "\n",
        "#オリジナル予測の為にラベルエンコーダーを関数の外で用意する\n",
        "#LE1,LE2,LE3 = LabelEncoder(),LabelEncoder(),LabelEncoder()\n",
        "\n",
        "\n",
        "#学習データ/テストデータを同時に前処理する関数\n",
        "#Real_Estate_Data_Preprocessing\n",
        "def REDP(d_df,d_t_df):\n",
        "    #バラバラになっているインデックスを振り直して整理する。\n",
        "    df_l = d_df.reset_index(drop=True)\n",
        "    df_t = d_t_df.reset_index(drop=True)\n",
        "    \n",
        "    #df内の余計な文字(\\n等)を消去する\n",
        "    for df in [df_l,df_t]:\n",
        "        df['礼金'] = df['礼金'].str.replace(r'\\(.+\\)','')\n",
        "        for element in ['住所','交通','専有面積','間取り','築年数','礼金','敷金']:\n",
        "            df[element] = df[element].str.translate(str.maketrans({'-':'','\\r':'','\\n':'','\\t':''\n",
        "                                                                   ,'バ':'','ス':'','分':'','徒':''\n",
        "                                                                   ,'歩':'','停':'','―':'','築':''\n",
        "                                                                   ,'年':'','新':'','万':'','(':''\n",
        "                                                                   ,'円':'','車':'','以':'','上':''}))\n",
        "    \n",
        "    #オリジナル予測の為にラベルエンコーダーをグローバル変数として用意する\n",
        "    global LE1,LE2,LE3\n",
        "    LE1,LE2,LE3 = LabelEncoder(),LabelEncoder(),LabelEncoder()\n",
        "    \n",
        "    #ラベルエンコーダーの学習を行い、列毎に変換の数字を合わせる\n",
        "    #注意:余計な文字を取り除いた段階でテストデータも一緒にラベルエンコードしないと、テストデータにのみ存在する住所等を変換できなくなってしまう為、学習データと縦に連結してfitさせる\n",
        "    LE_df = pd.concat([df_l,df_t])\n",
        "    LE1.fit(LE_df['住所'])\n",
        "    LE2.fit(LE_df['路線'])\n",
        "    LE3.fit(LE_df['間取り'])\n",
        "    \n",
        "    #ラベルエンコーディングを行い、文字列を数値化する\n",
        "    for df in [df_l,df_t]:\n",
        "        df['住所'] = LE1.transform(df['住所'])\n",
        "        df['路線'] = LE2.transform(df['路線'])\n",
        "        df['間取り'] = LE3.transform(df['間取り'])\n",
        "    \n",
        "        #欠損値を0に統一する\n",
        "        df.replace('',0,inplace=True)\n",
        "    \n",
        "    #データ型をfloat64で統一する\n",
        "    #astypeの使用上、for文に組み込めなかった\n",
        "    df_l = df_l.astype('float64')\n",
        "    df_t = df_t.astype('float64')\n",
        "    \n",
        "    #敷金・礼金の特徴量を「賃料のnヶ月分」として表す(そうしないとリークになってしまうから)\n",
        "    for name in ['敷金','礼金']:\n",
        "        for df in [df_l,df_t]:\n",
        "            df[name] = df[name] / df['賃料']\n",
        "    \n",
        "    return df_l,df_t\n",
        "\n",
        "\n",
        "#REDP(前処理したいリスト学習データ,前処理したいリストテストデータ)\n",
        "df_l,df_t = REDP(df,df_t)\n",
        "\n",
        "#確認用\n",
        "display(df_l.head(10))\n",
        "display(df_t.head(10))\n",
        "\n",
        "\n",
        "#オリジナル予測の為にラベルエンコーダーの学習した要素を変数に格納しておく\n",
        "adressC,stationC,layoutC = LE1.classes_, LE2.classes_ ,LE3.classes_\n",
        "print(adressC,'\\n','\\n',stationC,'\\n','\\n',layoutC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31ef7fa",
      "metadata": {
        "id": "e31ef7fa"
      },
      "outputs": [],
      "source": [
        "del df_l['路線']\n",
        "del df_t['路線']\n",
        "\n",
        "print(df_l.head(1))\n",
        "print(df_t.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d82c20",
      "metadata": {
        "id": "73d82c20"
      },
      "outputs": [],
      "source": [
        "#敷金・礼金の調整\n",
        "print(df_l['敷金'].unique())\n",
        "print(df_l['礼金'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764da609",
      "metadata": {
        "id": "764da609"
      },
      "outputs": [],
      "source": [
        "#XGBOOSTのモデルのクラス\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, params=None):\n",
        "        self.model = None\n",
        "        if params is None:\n",
        "            self.params = {}\n",
        "        else:\n",
        "            self.params = params\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "        params = {\n",
        "            #'booster': 'gbtree',\n",
        "            'objective': 'reg:squarederror',\n",
        "            'eta': 0.1,\n",
        "            'gamma': 0.0,\n",
        "            'alpha': 0.0,\n",
        "            'lambda': 1.0,\n",
        "            'min_child_weight': 1,\n",
        "            'max_depth': 5,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 71,\n",
        "        }\n",
        "        params.update(self.params)\n",
        "        num_round = 20\n",
        "        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
        "        dvalid = xgb.DMatrix(va_x, label=va_y)\n",
        "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
        "        self.model = xgb.train(params, dtrain, num_round, early_stopping_rounds=10, evals=watchlist)\n",
        "\n",
        "    def predict(self, x):\n",
        "        data = xgb.DMatrix(x)\n",
        "        pred = self.model.predict(data)\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2a0ba3",
      "metadata": {
        "scrolled": true,
        "id": "fa2a0ba3"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "\n",
        "# Modelクラスを定義しているものとする\n",
        "# Modelクラスは、fitで学習し、predictで予測値の確率を出力する\n",
        "\n",
        "scores = []\n",
        "models = []\n",
        "\n",
        "train_x = df_l.drop(['賃料'], axis=1)\n",
        "train_y = df_l['賃料']\n",
        "\n",
        "# KFoldクラスを用いてクロスバリデーションの分割を行う\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=71)\n",
        "for tr_idx, va_idx in kf.split(train_x):\n",
        "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
        "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
        "\n",
        "    # 学習の実行、バリデーションデータの予測値の出力、スコアの計算を行う\n",
        "    model = Model()\n",
        "    model.fit(tr_x, tr_y, va_x, va_y)\n",
        "    va_pred = model.predict(va_x)\n",
        "    score = np.sqrt(mean_squared_error(va_y,va_pred))\n",
        "    scores.append(score)\n",
        "    models.append(model)\n",
        "\n",
        "# 各foldのスコアの平均をとる\n",
        "print(np.mean(scores))\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f7c9d0",
      "metadata": {
        "id": "c3f7c9d0"
      },
      "outputs": [],
      "source": [
        "#4つのモデルを用いてアンサンブル予測\n",
        "\n",
        "test_x = df_t.drop(['賃料'], axis=1)\n",
        "test_y = df_t['賃料']\n",
        "pred_list = 0\n",
        "\n",
        "for model in models:\n",
        "    pred = model.predict(test_x)\n",
        "    pred_list += pred\n",
        "final_pred = pred_list / len(models)\n",
        "display(pd.DataFrame({'正解':test_y, \n",
        "                      '予測':final_pred}))\n",
        "\n",
        "print('MAE:',mean_absolute_error(test_y,final_pred))\n",
        "print('MSE:',mean_squared_error(test_y,final_pred))\n",
        "print('RMSE:',np.sqrt(mean_squared_error(test_y,final_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f24840",
      "metadata": {
        "id": "e7f24840"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリをインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "#hyperoptを使ったパラメータ探索\n",
        "def score(params):\n",
        "    # パラメータを与えたときに最小化する評価指標を指定する\n",
        "    # 具体的には、モデルにパラメータを指定して学習・予測させた場合のスコアを返すようにする\n",
        "\n",
        "    # max_depthの型を整数型に修正する\n",
        "    params['max_depth'] = int(params['max_depth'])\n",
        "\n",
        "    # Modelクラスを定義しているものとする\n",
        "    # Modelクラスは、fitで学習し、predictで予測値の確率を出力する\n",
        "    model = Model(params)\n",
        "    model.fit(tr_x, tr_y, va_x, va_y)\n",
        "    va_pred = model.predict(va_x)\n",
        "    score = np.sqrt(mean_squared_error(va_y, va_pred))\n",
        "    print(f'params: {params}, rmse: {score:.4f}')\n",
        "\n",
        "    # 情報を記録しておく\n",
        "    history.append((params, score))\n",
        "\n",
        "    return {'loss': score, 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "#データ毎にハイパーパラメータチューニングをhyperoptで行う。\n",
        "#HyperPramaterTuning_XGBOOST\n",
        "def HPT_XGB(df_l):\n",
        "    train_x = df_l.drop(['賃料'], axis=1)\n",
        "    train_y = df_l['賃料']\n",
        "\n",
        "    # 学習データを学習データとバリデーションデータに分ける\n",
        "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
        "    tr_idx, va_idx = list(kf.split(train_x))[0]\n",
        "    global tr_x,va_x,tr_y,va_y\n",
        "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
        "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
        "\n",
        "    # hp.choiceでは、複数の選択肢から選ぶ\n",
        "    # hp.uniformでは、下限・上限を指定した一様分布から抽出する。引数は下限・上限\n",
        "    # hp.quniformでは、下限・上限を指定した一様分布のうち一定の間隔ごとの点から抽出する。引数は下限・上限・間隔\n",
        "    # hp.loguniformでは、下限・上限を指定した対数が一様分布に従う分布から抽出する。引数は下限・上限の対数をとった値\n",
        "\n",
        "    # 探索するパラメータの空間を指定する\n",
        "    param_space = {\n",
        "        'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
        "        'min_child_weight': hp.loguniform('min_child_weight', np.log(0.1), np.log(10)),\n",
        "        'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
        "        'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
        "        'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
        "        'alpha' : hp.loguniform('alpha', np.log(1e-8), np.log(1.0)),\n",
        "        'lambda' : hp.loguniform('lambda', np.log(1e-6), np.log(10.0)),\n",
        "    }\n",
        "\n",
        "    # hyperoptによるパラメータ探索の実行\n",
        "    max_evals = 100\n",
        "    trials = Trials()\n",
        "    global history\n",
        "    history = []\n",
        "    fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
        "\n",
        "    # 記録した情報からパラメータとスコアを出力する\n",
        "    # （trialsからも情報が取得できるが、パラメータの取得がやや行いづらいため）\n",
        "    history = sorted(history, key=lambda tpl: tpl[1])\n",
        "    best = history[0]\n",
        "    print(\"\\n\",f'best params:{best[0]}, score:{best[1]:.4f}')\n",
        "\n",
        "    return best[0]\n",
        "\n",
        "best_params = HPT_XGB(df_l)\n",
        "#print(f'best_params:{best_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33570dc",
      "metadata": {
        "id": "e33570dc"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "import japanize_matplotlib #matplotlibでグラフを描写する際、日本語が文字化けするのを防ぐ。\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "import shap\n",
        "\n",
        "\n",
        "#pandasのデータフレームの列表示制限を解除\n",
        "pd.set_option('display.max_rows',None)\n",
        "\n",
        "\n",
        "#既にREDP関数で学習データ/テストデータの前処理が終わっているとする\n",
        "#df_l,df_t = REDP(d_list,d_t_list)\n",
        "\n",
        "\n",
        "#不動産賃料予測AIモデルの学習プログラム\n",
        "#HPT_XGBで取得したパラメータを利用する。\n",
        "#Real_Estate_Rent_Learning\n",
        "def RERL(df_l,best_params):\n",
        "    #データを説明変数と目的変数に分ける\n",
        "    target = df_l['賃料']\n",
        "    train = df_l.drop(['賃料'],axis=1)\n",
        "    \n",
        "    #ホールドアウト法で学習データとテストデータに分ける\n",
        "    kf = KFold(n_splits=4,shuffle=True,random_state=71)\n",
        "    tr_idx,va_idx = list(kf.split(train))[0]\n",
        "    tr_x,va_x = train.iloc[tr_idx],train.iloc[va_idx]\n",
        "    tr_y,va_y = target.iloc[tr_idx],target.iloc[va_idx]\n",
        "    \n",
        "    #データ型をXGBOOST用に適合させる\n",
        "    dtrain = xgb.DMatrix(tr_x,label=tr_y)\n",
        "    dvalid = xgb.DMatrix(va_x,label=va_y)\n",
        "    \n",
        "    #XGBOOSTのモデルを作成\n",
        "    num_round = 1000\n",
        "    early_stopping_rounds=20\n",
        "    #params = {'objective':'reg:squarederror','silent':1,'random_state':71}\n",
        "    watchlist = [(dtrain,'train'),(dvalid,'eval')]\n",
        "    model = xgb.train(best_params,dtrain,num_round,early_stopping_rounds=early_stopping_rounds,evals=watchlist)\n",
        "    \n",
        "    \n",
        "    #テストデータとその予測結果を表示\n",
        "    va_pred = model.predict(dvalid)\n",
        "    df_true = pd.DataFrame(list(va_y),columns=['賃料'])\n",
        "    df_pred = pd.DataFrame(va_pred,columns=['予測値'])\n",
        "    display(pd.concat([df_true,df_pred],axis=1))\n",
        "    #print(list(va_y))\n",
        "    #display(list(va_pred))\n",
        "    \n",
        "    #モデルの性能を表示\n",
        "    print('MAE:',mean_absolute_error(va_y,va_pred))\n",
        "    print('MSE:',mean_squared_error(va_y,va_pred))\n",
        "    print('RMSE:',np.sqrt(mean_squared_error(va_y,va_pred)))\n",
        "    \n",
        "    #変数の予測への寄与率をグラフにして表示\n",
        "    shap.initjs()\n",
        "    explainer = shap.TreeExplainer(model = model)\n",
        "    shap_values = explainer.shap_values(X = tr_x)\n",
        "    shap.summary_plot(shap_values, tr_x)\n",
        "    shap.summary_plot(shap_values, tr_x, plot_type = \"bar\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "#RERL(学習データ)\n",
        "model = RERL(df_l,best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f085a7ae",
      "metadata": {
        "id": "f085a7ae"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリをインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "#pandasのデータフレームの列表示制限を解除\n",
        "pd.set_option('display.max_rows',None)\n",
        "\n",
        "\n",
        "#既に学習データとREDP関数を用いてAIが作成されているものとする\n",
        "#model = RERL(df_l)\n",
        "\n",
        "\n",
        "#RERLで作ったAIを用いて、テストデータの不動産の賃料を予測する関数\n",
        "#Real_Estate_Rent_Divided_Prediction\n",
        "def RERDP(df_t):\n",
        "    #学習データと教師データの分割\n",
        "    df_fact = df_t['賃料']\n",
        "    df_input = df_t.drop(['賃料'],axis=1)\n",
        "    \n",
        "    #予測値の生成\n",
        "    df_input = xgb.DMatrix(df_input)\n",
        "    pred = model.predict(df_input)\n",
        "    \n",
        "    #実値と予測値の表示\n",
        "    df_pred = pd.DataFrame(pred,columns=['予測値'])\n",
        "    df_final = pd.concat([df_fact,df_pred],axis=1)\n",
        "    display(df_final)\n",
        "    \n",
        "    #モデルの性能を表示\n",
        "    print('MAE:',mean_absolute_error(df_fact,df_pred))\n",
        "    print('MSE:',mean_squared_error(df_fact,df_pred))\n",
        "    print('RMSE:',np.sqrt(mean_squared_error(df_fact,df_pred)))\n",
        "\n",
        "\n",
        "#RERDP(テストデータ)\n",
        "RERDP(df_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939716ab",
      "metadata": {
        "id": "939716ab"
      },
      "outputs": [],
      "source": [
        "#目的変数とのクロス集計プログラム\n",
        "def all_plot(df_final):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import japanize_matplotlib\n",
        "    #from sklearn.linear_model import LinearRegression\n",
        "    \n",
        "    y = df_final['賃料']\n",
        "    columns = list(df_final.columns)\n",
        "    \n",
        "    fig, ax = plt.subplots(len(columns),1,figsize=(40,30*len(columns)))\n",
        "    plt.rcParams[\"font.size\"] = 30\n",
        "\n",
        "    for i,column in enumerate(columns):\n",
        "        x = df_final[column]\n",
        "        \n",
        "        ax[i].set_title(column)\n",
        "\n",
        "        ax[i].scatter(x,y)\n",
        "        \n",
        "        print(column,'の相関係数：',x.corr(y))\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90682bb1",
      "metadata": {
        "scrolled": true,
        "id": "90682bb1"
      },
      "outputs": [],
      "source": [
        "all_plot(df_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afa8ab5",
      "metadata": {
        "id": "5afa8ab5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import seaborn as sns \n",
        "plt.figure(figsize=(2, 2))\n",
        "sns.pairplot(df_l, size=2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "674acfcc",
      "metadata": {
        "id": "674acfcc"
      },
      "outputs": [],
      "source": [
        "#df_lの分散・共分散行列を見やすいヒートマップ形式にて出力\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import seaborn as sns \n",
        "plt.rcParams[\"font.size\"] = 10\n",
        "sns.heatmap(df_l.corr(), annot=True, fmt='.2f')\n",
        "#説明変数同士の相関は強くないため、多重共線性の心配はない事が分かる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af71a86",
      "metadata": {
        "id": "3af71a86"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリをインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "#numpyのリスト表示制限を解除しておく\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "\n",
        "#既に学習データとREDP関数を用いてAIが作成されているものとする\n",
        "#model = RERL(df_l)\n",
        "\n",
        "\n",
        "#住所と路線と間取りはラベルエンコーディングの都合により学習データ/テストデータにあったものしか使えない為、予め確保しておいた使える要素を表示させる\n",
        "#上記の3つの要素はこの中から選んでもらう\n",
        "#このプログラムは別のセルで起動させると見やすい\n",
        "print(adressC,'\\n','\\n',stationC,'\\n','\\n',layoutC)\n",
        "\n",
        "\n",
        "#(学習した範囲内の)任意のデータを入力して賃料を予測できる関数\n",
        "#Real_Estate_Own_Data_Prediction\n",
        "def REODP(address,station,access,mc_fees,k_fees,s_fees,area,layout,age):\n",
        "    #入力したデータを辞書d_tryに格納する\n",
        "    d_try = {\n",
        "        '住所':address,\n",
        "        '路線':station,\n",
        "        '交通':access,\n",
        "        '管理共益費':mc_fees,\n",
        "        '礼金':k_fees,\n",
        "        '敷金':s_fees,\n",
        "        '専有面積':area,\n",
        "        '間取り':layout,\n",
        "        '築年数':age\n",
        "    }\n",
        "    \n",
        "    #辞書d_tryをデータフレームdf_tryに変換する\n",
        "    df_try = pd.DataFrame(d_try,index=['own'])\n",
        "    \n",
        "    #入力情報の確認用\n",
        "    display(df_try)\n",
        "    \n",
        "    #ラベルエンコーディングを行い、文字列を数値化する\n",
        "    df_try.住所 = LE1.transform(df_try.住所)\n",
        "    df_try.路線 = LE2.transform(df_try.路線)\n",
        "    df_try.間取り = LE3.transform(df_try.間取り)\n",
        "    \n",
        "    #データ型をfloat64で統一する\n",
        "    df_try = df_try.astype('float64')\n",
        "    \n",
        "    #予測結果を表示する\n",
        "    df_try = xgb.DMatrix(df_try)\n",
        "    return print('予想賃料:',float(model.predict(df_try)),'万円')\n",
        "\n",
        "\n",
        "#REODP(住所, 路線, 交通, 管理共益費, 礼金, 敷金, 専有面積, 間取り, 築年数)\n",
        "#データ型に気をつける\n",
        "#住所と路線と間取りはラベルエンコーディングの都合により学習データ/テストデータにあったものしか使えない為、上で表示させた要素から選ぶこと\n",
        "REODP(address='神奈川県相模原市中央区淵野辺５'\n",
        "      ,station='ＪＲ横浜線/相模原'\n",
        "      ,access=15\n",
        "      ,mc_fees=5000\n",
        "      ,k_fees=0\n",
        "      ,s_fees=0\n",
        "      ,area=90\n",
        "      ,layout='4LDK'\n",
        "      ,age=19\n",
        "     )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eefeede",
      "metadata": {
        "id": "1eefeede"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "SUUMOスクレイピング_改善.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}